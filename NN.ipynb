{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>tavg</th>\n",
       "      <th>tmin</th>\n",
       "      <th>tmax</th>\n",
       "      <th>prcp</th>\n",
       "      <th>wspd</th>\n",
       "      <th>pres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>13.1</td>\n",
       "      <td>8.3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>1019.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>14.4</td>\n",
       "      <td>8.3</td>\n",
       "      <td>21.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1017.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-01-03</td>\n",
       "      <td>14.2</td>\n",
       "      <td>7.8</td>\n",
       "      <td>18.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1015.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-01-04</td>\n",
       "      <td>15.4</td>\n",
       "      <td>13.3</td>\n",
       "      <td>18.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1015.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-01-05</td>\n",
       "      <td>15.4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1018.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         time  tavg  tmin  tmax  prcp  wspd    pres\n",
       "0  2014-01-01  13.1   8.3  20.0   0.0   5.4  1019.1\n",
       "1  2014-01-02  14.4   8.3  21.7   0.0   4.3  1017.9\n",
       "2  2014-01-03  14.2   7.8  18.9   0.0   3.2  1015.7\n",
       "3  2014-01-04  15.4  13.3  18.9   0.0   2.9  1015.6\n",
       "4  2014-01-05  15.4  10.0  23.3   0.0   5.0  1018.8"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"weatherdata.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>tavg</th>\n",
       "      <th>prcp</th>\n",
       "      <th>wspd</th>\n",
       "      <th>pres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>13.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>1019.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>14.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1017.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-01-03</td>\n",
       "      <td>14.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1015.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-01-04</td>\n",
       "      <td>15.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1015.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-01-05</td>\n",
       "      <td>15.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1018.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         time  tavg  prcp  wspd    pres\n",
       "0  2014-01-01  13.1   0.0   5.4  1019.1\n",
       "1  2014-01-02  14.4   0.0   4.3  1017.9\n",
       "2  2014-01-03  14.2   0.0   3.2  1015.7\n",
       "3  2014-01-04  15.4   0.0   2.9  1015.6\n",
       "4  2014-01-05  15.4   0.0   5.0  1018.8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.drop(['tmin','tmax'],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>tavg</th>\n",
       "      <th>prcp</th>\n",
       "      <th>wspd</th>\n",
       "      <th>pres</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>13.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>1019.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>14.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1017.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-01-03</td>\n",
       "      <td>14.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1015.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-01-04</td>\n",
       "      <td>15.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1015.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-01-05</td>\n",
       "      <td>15.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1018.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        time  tavg  prcp  wspd    pres  month\n",
       "0 2014-01-01  13.1   0.0   5.4  1019.1      1\n",
       "1 2014-01-02  14.4   0.0   4.3  1017.9      1\n",
       "2 2014-01-03  14.2   0.0   3.2  1015.7      1\n",
       "3 2014-01-04  15.4   0.0   2.9  1015.6      1\n",
       "4 2014-01-05  15.4   0.0   5.0  1018.8      1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['time']=pd.to_datetime(df['time'])\n",
    "df['month']=df['time'].dt.month\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df[['month','prcp','wspd','pres']]\n",
    "y=df['tavg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain,Xtest,ytrain,ytest=train_test_split(X,y,test_size=0.2,random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "Xtrain=scaler.fit_transform(Xtrain)\n",
    "Xtest=scaler.fit_transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain=np.array(ytrain)\n",
    "ytest=np.array(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain=torch.tensor(Xtrain)\n",
    "Xtest=torch.tensor(Xtest)\n",
    "ytrain=torch.tensor(ytrain).view(-1,1)\n",
    "ytest=torch.tensor(ytest).view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model definition\n",
    "\n",
    "model=nn.Sequential(\n",
    "    nn.Linear(4,64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64,64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64,1)\n",
    ")\n",
    "\n",
    "criterion=nn.MSELoss()\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 0 loss : 347.4424133300781\n",
      "iteration : 1 loss : 346.03070068359375\n",
      "iteration : 2 loss : 344.6383972167969\n",
      "iteration : 3 loss : 343.2633056640625\n",
      "iteration : 4 loss : 341.89898681640625\n",
      "iteration : 5 loss : 340.5382080078125\n",
      "iteration : 6 loss : 339.1736145019531\n",
      "iteration : 7 loss : 337.79815673828125\n",
      "iteration : 8 loss : 336.4078674316406\n",
      "iteration : 9 loss : 335.0029296875\n",
      "iteration : 10 loss : 333.5831298828125\n",
      "iteration : 11 loss : 332.1460876464844\n",
      "iteration : 12 loss : 330.69085693359375\n",
      "iteration : 13 loss : 329.2158203125\n",
      "iteration : 14 loss : 327.7165832519531\n",
      "iteration : 15 loss : 326.18902587890625\n",
      "iteration : 16 loss : 324.62841796875\n",
      "iteration : 17 loss : 323.0315246582031\n",
      "iteration : 18 loss : 321.3956604003906\n",
      "iteration : 19 loss : 319.7181396484375\n",
      "iteration : 20 loss : 317.9952392578125\n",
      "iteration : 21 loss : 316.22772216796875\n",
      "iteration : 22 loss : 314.413818359375\n",
      "iteration : 23 loss : 312.55279541015625\n",
      "iteration : 24 loss : 310.64288330078125\n",
      "iteration : 25 loss : 308.6826477050781\n",
      "iteration : 26 loss : 306.669189453125\n",
      "iteration : 27 loss : 304.60137939453125\n",
      "iteration : 28 loss : 302.4785461425781\n",
      "iteration : 29 loss : 300.3004150390625\n",
      "iteration : 30 loss : 298.0655822753906\n",
      "iteration : 31 loss : 295.7730712890625\n",
      "iteration : 32 loss : 293.421875\n",
      "iteration : 33 loss : 291.0118408203125\n",
      "iteration : 34 loss : 288.5428466796875\n",
      "iteration : 35 loss : 286.01458740234375\n",
      "iteration : 36 loss : 283.4267578125\n",
      "iteration : 37 loss : 280.7785339355469\n",
      "iteration : 38 loss : 278.07000732421875\n",
      "iteration : 39 loss : 275.30059814453125\n",
      "iteration : 40 loss : 272.4704895019531\n",
      "iteration : 41 loss : 269.5795593261719\n",
      "iteration : 42 loss : 266.6279296875\n",
      "iteration : 43 loss : 263.6149597167969\n",
      "iteration : 44 loss : 260.5406188964844\n",
      "iteration : 45 loss : 257.4063415527344\n",
      "iteration : 46 loss : 254.21348571777344\n",
      "iteration : 47 loss : 250.9642791748047\n",
      "iteration : 48 loss : 247.6600341796875\n",
      "iteration : 49 loss : 244.30184936523438\n",
      "iteration : 50 loss : 240.8910369873047\n",
      "iteration : 51 loss : 237.42893981933594\n",
      "iteration : 52 loss : 233.91702270507812\n",
      "iteration : 53 loss : 230.35678100585938\n",
      "iteration : 54 loss : 226.74978637695312\n",
      "iteration : 55 loss : 223.09780883789062\n",
      "iteration : 56 loss : 219.4026641845703\n",
      "iteration : 57 loss : 215.6659393310547\n",
      "iteration : 58 loss : 211.89024353027344\n",
      "iteration : 59 loss : 208.07815551757812\n",
      "iteration : 60 loss : 204.23275756835938\n",
      "iteration : 61 loss : 200.3571014404297\n",
      "iteration : 62 loss : 196.4539031982422\n",
      "iteration : 63 loss : 192.52615356445312\n",
      "iteration : 64 loss : 188.57691955566406\n",
      "iteration : 65 loss : 184.6092529296875\n",
      "iteration : 66 loss : 180.6266632080078\n",
      "iteration : 67 loss : 176.6327362060547\n",
      "iteration : 68 loss : 172.63124084472656\n",
      "iteration : 69 loss : 168.6262664794922\n",
      "iteration : 70 loss : 164.62184143066406\n",
      "iteration : 71 loss : 160.62191772460938\n",
      "iteration : 72 loss : 156.63058471679688\n",
      "iteration : 73 loss : 152.6519012451172\n",
      "iteration : 74 loss : 148.6903839111328\n",
      "iteration : 75 loss : 144.7504425048828\n",
      "iteration : 76 loss : 140.8369903564453\n",
      "iteration : 77 loss : 136.95452880859375\n",
      "iteration : 78 loss : 133.10757446289062\n",
      "iteration : 79 loss : 129.3010711669922\n",
      "iteration : 80 loss : 125.53955841064453\n",
      "iteration : 81 loss : 121.82772827148438\n",
      "iteration : 82 loss : 118.17030334472656\n",
      "iteration : 83 loss : 114.57160949707031\n",
      "iteration : 84 loss : 111.03553771972656\n",
      "iteration : 85 loss : 107.56633758544922\n",
      "iteration : 86 loss : 104.16847229003906\n",
      "iteration : 87 loss : 100.84635162353516\n",
      "iteration : 88 loss : 97.6032485961914\n",
      "iteration : 89 loss : 94.44261169433594\n",
      "iteration : 90 loss : 91.3679428100586\n",
      "iteration : 91 loss : 88.38208770751953\n",
      "iteration : 92 loss : 85.48725891113281\n",
      "iteration : 93 loss : 82.68611145019531\n",
      "iteration : 94 loss : 79.98097229003906\n",
      "iteration : 95 loss : 77.37345123291016\n",
      "iteration : 96 loss : 74.8653564453125\n",
      "iteration : 97 loss : 72.45674896240234\n",
      "iteration : 98 loss : 70.14813995361328\n",
      "iteration : 99 loss : 67.939208984375\n",
      "iteration : 100 loss : 65.82987976074219\n",
      "iteration : 101 loss : 63.81920623779297\n",
      "iteration : 102 loss : 61.90619659423828\n",
      "iteration : 103 loss : 60.089115142822266\n",
      "iteration : 104 loss : 58.36599349975586\n",
      "iteration : 105 loss : 56.73488998413086\n",
      "iteration : 106 loss : 55.19273376464844\n",
      "iteration : 107 loss : 53.7367057800293\n",
      "iteration : 108 loss : 52.363922119140625\n",
      "iteration : 109 loss : 51.07087326049805\n",
      "iteration : 110 loss : 49.85394287109375\n",
      "iteration : 111 loss : 48.709632873535156\n",
      "iteration : 112 loss : 47.6336555480957\n",
      "iteration : 113 loss : 46.62174987792969\n",
      "iteration : 114 loss : 45.670345306396484\n",
      "iteration : 115 loss : 44.77539825439453\n",
      "iteration : 116 loss : 43.932838439941406\n",
      "iteration : 117 loss : 43.138824462890625\n",
      "iteration : 118 loss : 42.389949798583984\n",
      "iteration : 119 loss : 41.6829719543457\n",
      "iteration : 120 loss : 41.01453399658203\n",
      "iteration : 121 loss : 40.38108444213867\n",
      "iteration : 122 loss : 39.77955627441406\n",
      "iteration : 123 loss : 39.207000732421875\n",
      "iteration : 124 loss : 38.66116714477539\n",
      "iteration : 125 loss : 38.139469146728516\n",
      "iteration : 126 loss : 37.63973617553711\n",
      "iteration : 127 loss : 37.159812927246094\n",
      "iteration : 128 loss : 36.698455810546875\n",
      "iteration : 129 loss : 36.253753662109375\n",
      "iteration : 130 loss : 35.824153900146484\n",
      "iteration : 131 loss : 35.408607482910156\n",
      "iteration : 132 loss : 35.006065368652344\n",
      "iteration : 133 loss : 34.61585235595703\n",
      "iteration : 134 loss : 34.237361907958984\n",
      "iteration : 135 loss : 33.869468688964844\n",
      "iteration : 136 loss : 33.511417388916016\n",
      "iteration : 137 loss : 33.162933349609375\n",
      "iteration : 138 loss : 32.82373046875\n",
      "iteration : 139 loss : 32.49314498901367\n",
      "iteration : 140 loss : 32.17109298706055\n",
      "iteration : 141 loss : 31.85689353942871\n",
      "iteration : 142 loss : 31.550243377685547\n",
      "iteration : 143 loss : 31.251123428344727\n",
      "iteration : 144 loss : 30.95907974243164\n",
      "iteration : 145 loss : 30.674152374267578\n",
      "iteration : 146 loss : 30.39603042602539\n",
      "iteration : 147 loss : 30.12432098388672\n",
      "iteration : 148 loss : 29.85866928100586\n",
      "iteration : 149 loss : 29.598968505859375\n",
      "iteration : 150 loss : 29.34493637084961\n",
      "iteration : 151 loss : 29.095972061157227\n",
      "iteration : 152 loss : 28.852020263671875\n",
      "iteration : 153 loss : 28.613101959228516\n",
      "iteration : 154 loss : 28.37919044494629\n",
      "iteration : 155 loss : 28.149919509887695\n",
      "iteration : 156 loss : 27.925077438354492\n",
      "iteration : 157 loss : 27.704574584960938\n",
      "iteration : 158 loss : 27.48814582824707\n",
      "iteration : 159 loss : 27.275712966918945\n",
      "iteration : 160 loss : 27.067668914794922\n",
      "iteration : 161 loss : 26.863374710083008\n",
      "iteration : 162 loss : 26.662845611572266\n",
      "iteration : 163 loss : 26.465896606445312\n",
      "iteration : 164 loss : 26.272624969482422\n",
      "iteration : 165 loss : 26.082536697387695\n",
      "iteration : 166 loss : 25.89463233947754\n",
      "iteration : 167 loss : 25.708688735961914\n",
      "iteration : 168 loss : 25.524682998657227\n",
      "iteration : 169 loss : 25.342517852783203\n",
      "iteration : 170 loss : 25.162214279174805\n",
      "iteration : 171 loss : 24.983854293823242\n",
      "iteration : 172 loss : 24.807331085205078\n",
      "iteration : 173 loss : 24.632532119750977\n",
      "iteration : 174 loss : 24.459365844726562\n",
      "iteration : 175 loss : 24.287797927856445\n",
      "iteration : 176 loss : 24.117727279663086\n",
      "iteration : 177 loss : 23.949220657348633\n",
      "iteration : 178 loss : 23.782588958740234\n",
      "iteration : 179 loss : 23.617826461791992\n",
      "iteration : 180 loss : 23.45526885986328\n",
      "iteration : 181 loss : 23.294790267944336\n",
      "iteration : 182 loss : 23.1363525390625\n",
      "iteration : 183 loss : 22.97970199584961\n",
      "iteration : 184 loss : 22.8245906829834\n",
      "iteration : 185 loss : 22.67093849182129\n",
      "iteration : 186 loss : 22.518564224243164\n",
      "iteration : 187 loss : 22.36717987060547\n",
      "iteration : 188 loss : 22.21680450439453\n",
      "iteration : 189 loss : 22.06747055053711\n",
      "iteration : 190 loss : 21.919090270996094\n",
      "iteration : 191 loss : 21.77172088623047\n",
      "iteration : 192 loss : 21.625391006469727\n",
      "iteration : 193 loss : 21.48004150390625\n",
      "iteration : 194 loss : 21.335599899291992\n",
      "iteration : 195 loss : 21.192358016967773\n",
      "iteration : 196 loss : 21.050201416015625\n",
      "iteration : 197 loss : 20.90915298461914\n",
      "iteration : 198 loss : 20.76914405822754\n",
      "iteration : 199 loss : 20.63018035888672\n",
      "iteration : 200 loss : 20.492273330688477\n",
      "iteration : 201 loss : 20.35540008544922\n",
      "iteration : 202 loss : 20.219432830810547\n",
      "iteration : 203 loss : 20.084348678588867\n",
      "iteration : 204 loss : 19.950151443481445\n",
      "iteration : 205 loss : 19.816909790039062\n",
      "iteration : 206 loss : 19.684627532958984\n",
      "iteration : 207 loss : 19.55325698852539\n",
      "iteration : 208 loss : 19.422752380371094\n",
      "iteration : 209 loss : 19.29311180114746\n",
      "iteration : 210 loss : 19.16434097290039\n",
      "iteration : 211 loss : 19.036447525024414\n",
      "iteration : 212 loss : 18.9094181060791\n",
      "iteration : 213 loss : 18.783275604248047\n",
      "iteration : 214 loss : 18.658111572265625\n",
      "iteration : 215 loss : 18.533788681030273\n",
      "iteration : 216 loss : 18.410293579101562\n",
      "iteration : 217 loss : 18.287538528442383\n",
      "iteration : 218 loss : 18.165626525878906\n",
      "iteration : 219 loss : 18.044553756713867\n",
      "iteration : 220 loss : 17.924232482910156\n",
      "iteration : 221 loss : 17.80461311340332\n",
      "iteration : 222 loss : 17.685678482055664\n",
      "iteration : 223 loss : 17.56736946105957\n",
      "iteration : 224 loss : 17.4497127532959\n",
      "iteration : 225 loss : 17.33268165588379\n",
      "iteration : 226 loss : 17.216270446777344\n",
      "iteration : 227 loss : 17.10053253173828\n",
      "iteration : 228 loss : 16.985492706298828\n",
      "iteration : 229 loss : 16.871171951293945\n",
      "iteration : 230 loss : 16.757448196411133\n",
      "iteration : 231 loss : 16.644357681274414\n",
      "iteration : 232 loss : 16.53192710876465\n",
      "iteration : 233 loss : 16.420093536376953\n",
      "iteration : 234 loss : 16.30886459350586\n",
      "iteration : 235 loss : 16.198318481445312\n",
      "iteration : 236 loss : 16.088342666625977\n",
      "iteration : 237 loss : 15.978918075561523\n",
      "iteration : 238 loss : 15.870001792907715\n",
      "iteration : 239 loss : 15.761499404907227\n",
      "iteration : 240 loss : 15.653615951538086\n",
      "iteration : 241 loss : 15.546342849731445\n",
      "iteration : 242 loss : 15.43966007232666\n",
      "iteration : 243 loss : 15.333511352539062\n",
      "iteration : 244 loss : 15.227773666381836\n",
      "iteration : 245 loss : 15.122530937194824\n",
      "iteration : 246 loss : 15.017860412597656\n",
      "iteration : 247 loss : 14.913642883300781\n",
      "iteration : 248 loss : 14.809914588928223\n",
      "iteration : 249 loss : 14.706744194030762\n",
      "iteration : 250 loss : 14.604147911071777\n",
      "iteration : 251 loss : 14.501967430114746\n",
      "iteration : 252 loss : 14.400287628173828\n",
      "iteration : 253 loss : 14.299138069152832\n",
      "iteration : 254 loss : 14.198554039001465\n",
      "iteration : 255 loss : 14.098477363586426\n",
      "iteration : 256 loss : 13.99898910522461\n",
      "iteration : 257 loss : 13.899968147277832\n",
      "iteration : 258 loss : 13.801433563232422\n",
      "iteration : 259 loss : 13.703295707702637\n",
      "iteration : 260 loss : 13.605508804321289\n",
      "iteration : 261 loss : 13.508098602294922\n",
      "iteration : 262 loss : 13.411197662353516\n",
      "iteration : 263 loss : 13.314772605895996\n",
      "iteration : 264 loss : 13.218709945678711\n",
      "iteration : 265 loss : 13.12308406829834\n",
      "iteration : 266 loss : 13.027759552001953\n",
      "iteration : 267 loss : 12.932788848876953\n",
      "iteration : 268 loss : 12.838220596313477\n",
      "iteration : 269 loss : 12.744119644165039\n",
      "iteration : 270 loss : 12.650420188903809\n",
      "iteration : 271 loss : 12.557077407836914\n",
      "iteration : 272 loss : 12.464158058166504\n",
      "iteration : 273 loss : 12.371668815612793\n",
      "iteration : 274 loss : 12.27957534790039\n",
      "iteration : 275 loss : 12.18786907196045\n",
      "iteration : 276 loss : 12.096628189086914\n",
      "iteration : 277 loss : 12.005836486816406\n",
      "iteration : 278 loss : 11.915420532226562\n",
      "iteration : 279 loss : 11.825392723083496\n",
      "iteration : 280 loss : 11.735716819763184\n",
      "iteration : 281 loss : 11.646368026733398\n",
      "iteration : 282 loss : 11.557265281677246\n",
      "iteration : 283 loss : 11.468632698059082\n",
      "iteration : 284 loss : 11.380537033081055\n",
      "iteration : 285 loss : 11.292936325073242\n",
      "iteration : 286 loss : 11.205984115600586\n",
      "iteration : 287 loss : 11.11970329284668\n",
      "iteration : 288 loss : 11.034183502197266\n",
      "iteration : 289 loss : 10.949456214904785\n",
      "iteration : 290 loss : 10.86542797088623\n",
      "iteration : 291 loss : 10.782073020935059\n",
      "iteration : 292 loss : 10.699374198913574\n",
      "iteration : 293 loss : 10.617349624633789\n",
      "iteration : 294 loss : 10.535991668701172\n",
      "iteration : 295 loss : 10.455278396606445\n",
      "iteration : 296 loss : 10.375246047973633\n",
      "iteration : 297 loss : 10.295890808105469\n",
      "iteration : 298 loss : 10.217228889465332\n",
      "iteration : 299 loss : 10.13922119140625\n",
      "iteration : 300 loss : 10.061933517456055\n",
      "iteration : 301 loss : 9.985337257385254\n",
      "iteration : 302 loss : 9.909424781799316\n",
      "iteration : 303 loss : 9.83420467376709\n",
      "iteration : 304 loss : 9.7597074508667\n",
      "iteration : 305 loss : 9.685943603515625\n",
      "iteration : 306 loss : 9.612871170043945\n",
      "iteration : 307 loss : 9.540538787841797\n",
      "iteration : 308 loss : 9.46894645690918\n",
      "iteration : 309 loss : 9.398163795471191\n",
      "iteration : 310 loss : 9.328117370605469\n",
      "iteration : 311 loss : 9.258769989013672\n",
      "iteration : 312 loss : 9.190142631530762\n",
      "iteration : 313 loss : 9.122270584106445\n",
      "iteration : 314 loss : 9.05516242980957\n",
      "iteration : 315 loss : 8.988785743713379\n",
      "iteration : 316 loss : 8.92313289642334\n",
      "iteration : 317 loss : 8.85823917388916\n",
      "iteration : 318 loss : 8.794057846069336\n",
      "iteration : 319 loss : 8.730646133422852\n",
      "iteration : 320 loss : 8.66800594329834\n",
      "iteration : 321 loss : 8.606148719787598\n",
      "iteration : 322 loss : 8.545034408569336\n",
      "iteration : 323 loss : 8.484670639038086\n",
      "iteration : 324 loss : 8.425063133239746\n",
      "iteration : 325 loss : 8.366158485412598\n",
      "iteration : 326 loss : 8.308005332946777\n",
      "iteration : 327 loss : 8.250584602355957\n",
      "iteration : 328 loss : 8.19388484954834\n",
      "iteration : 329 loss : 8.137895584106445\n",
      "iteration : 330 loss : 8.082547187805176\n",
      "iteration : 331 loss : 8.027891159057617\n",
      "iteration : 332 loss : 7.973938941955566\n",
      "iteration : 333 loss : 7.920665740966797\n",
      "iteration : 334 loss : 7.868067741394043\n",
      "iteration : 335 loss : 7.816188335418701\n",
      "iteration : 336 loss : 7.764974594116211\n",
      "iteration : 337 loss : 7.714418411254883\n",
      "iteration : 338 loss : 7.664457321166992\n",
      "iteration : 339 loss : 7.615087509155273\n",
      "iteration : 340 loss : 7.566376686096191\n",
      "iteration : 341 loss : 7.518319606781006\n",
      "iteration : 342 loss : 7.4708943367004395\n",
      "iteration : 343 loss : 7.42402982711792\n",
      "iteration : 344 loss : 7.37767219543457\n",
      "iteration : 345 loss : 7.331959247589111\n",
      "iteration : 346 loss : 7.286786079406738\n",
      "iteration : 347 loss : 7.242122173309326\n",
      "iteration : 348 loss : 7.19806432723999\n",
      "iteration : 349 loss : 7.154636383056641\n",
      "iteration : 350 loss : 7.111815452575684\n",
      "iteration : 351 loss : 7.0696916580200195\n",
      "iteration : 352 loss : 7.028393268585205\n",
      "iteration : 353 loss : 6.987715721130371\n",
      "iteration : 354 loss : 6.947721481323242\n",
      "iteration : 355 loss : 6.908385753631592\n",
      "iteration : 356 loss : 6.869729995727539\n",
      "iteration : 357 loss : 6.831876754760742\n",
      "iteration : 358 loss : 6.794765472412109\n",
      "iteration : 359 loss : 6.758388519287109\n",
      "iteration : 360 loss : 6.722672462463379\n",
      "iteration : 361 loss : 6.687561988830566\n",
      "iteration : 362 loss : 6.653052806854248\n",
      "iteration : 363 loss : 6.619140148162842\n",
      "iteration : 364 loss : 6.585857391357422\n",
      "iteration : 365 loss : 6.553183078765869\n",
      "iteration : 366 loss : 6.521090030670166\n",
      "iteration : 367 loss : 6.489565372467041\n",
      "iteration : 368 loss : 6.45862340927124\n",
      "iteration : 369 loss : 6.428225517272949\n",
      "iteration : 370 loss : 6.398379802703857\n",
      "iteration : 371 loss : 6.369086265563965\n",
      "iteration : 372 loss : 6.340381622314453\n",
      "iteration : 373 loss : 6.312293529510498\n",
      "iteration : 374 loss : 6.284659385681152\n",
      "iteration : 375 loss : 6.257507801055908\n",
      "iteration : 376 loss : 6.230778694152832\n",
      "iteration : 377 loss : 6.20453405380249\n",
      "iteration : 378 loss : 6.178730487823486\n",
      "iteration : 379 loss : 6.153353691101074\n",
      "iteration : 380 loss : 6.1283650398254395\n",
      "iteration : 381 loss : 6.10378885269165\n",
      "iteration : 382 loss : 6.079610824584961\n",
      "iteration : 383 loss : 6.0558342933654785\n",
      "iteration : 384 loss : 6.0324506759643555\n",
      "iteration : 385 loss : 6.009455680847168\n",
      "iteration : 386 loss : 5.9868035316467285\n",
      "iteration : 387 loss : 5.96449613571167\n",
      "iteration : 388 loss : 5.9425153732299805\n",
      "iteration : 389 loss : 5.920883655548096\n",
      "iteration : 390 loss : 5.899568557739258\n",
      "iteration : 391 loss : 5.878584861755371\n",
      "iteration : 392 loss : 5.857922077178955\n",
      "iteration : 393 loss : 5.837531089782715\n",
      "iteration : 394 loss : 5.817399024963379\n",
      "iteration : 395 loss : 5.79755163192749\n",
      "iteration : 396 loss : 5.778007984161377\n",
      "iteration : 397 loss : 5.7587080001831055\n",
      "iteration : 398 loss : 5.739650249481201\n",
      "iteration : 399 loss : 5.72081184387207\n",
      "iteration : 400 loss : 5.702149868011475\n",
      "iteration : 401 loss : 5.683684825897217\n",
      "iteration : 402 loss : 5.665427207946777\n",
      "iteration : 403 loss : 5.647377967834473\n",
      "iteration : 404 loss : 5.629509925842285\n",
      "iteration : 405 loss : 5.6118669509887695\n",
      "iteration : 406 loss : 5.594439506530762\n",
      "iteration : 407 loss : 5.5772199630737305\n",
      "iteration : 408 loss : 5.560190677642822\n",
      "iteration : 409 loss : 5.543272018432617\n",
      "iteration : 410 loss : 5.52650260925293\n",
      "iteration : 411 loss : 5.5098981857299805\n",
      "iteration : 412 loss : 5.493448257446289\n",
      "iteration : 413 loss : 5.477123260498047\n",
      "iteration : 414 loss : 5.460960388183594\n",
      "iteration : 415 loss : 5.444894313812256\n",
      "iteration : 416 loss : 5.428915977478027\n",
      "iteration : 417 loss : 5.4130539894104\n",
      "iteration : 418 loss : 5.3972487449646\n",
      "iteration : 419 loss : 5.381572723388672\n",
      "iteration : 420 loss : 5.3660359382629395\n",
      "iteration : 421 loss : 5.350618839263916\n",
      "iteration : 422 loss : 5.335283279418945\n",
      "iteration : 423 loss : 5.320021629333496\n",
      "iteration : 424 loss : 5.3048882484436035\n",
      "iteration : 425 loss : 5.289867877960205\n",
      "iteration : 426 loss : 5.274918556213379\n",
      "iteration : 427 loss : 5.259922027587891\n",
      "iteration : 428 loss : 5.244855880737305\n",
      "iteration : 429 loss : 5.229753017425537\n",
      "iteration : 430 loss : 5.2146100997924805\n",
      "iteration : 431 loss : 5.199497222900391\n",
      "iteration : 432 loss : 5.1844940185546875\n",
      "iteration : 433 loss : 5.1696062088012695\n",
      "iteration : 434 loss : 5.154780864715576\n",
      "iteration : 435 loss : 5.140157699584961\n",
      "iteration : 436 loss : 5.125733375549316\n",
      "iteration : 437 loss : 5.111435890197754\n",
      "iteration : 438 loss : 5.09727668762207\n",
      "iteration : 439 loss : 5.083217620849609\n",
      "iteration : 440 loss : 5.069484233856201\n",
      "iteration : 441 loss : 5.055912494659424\n",
      "iteration : 442 loss : 5.042489051818848\n",
      "iteration : 443 loss : 5.029202938079834\n",
      "iteration : 444 loss : 5.016074180603027\n",
      "iteration : 445 loss : 5.003204345703125\n",
      "iteration : 446 loss : 4.990574836730957\n",
      "iteration : 447 loss : 4.978178977966309\n",
      "iteration : 448 loss : 4.966145992279053\n",
      "iteration : 449 loss : 4.954465389251709\n",
      "iteration : 450 loss : 4.9429545402526855\n",
      "iteration : 451 loss : 4.931581497192383\n",
      "iteration : 452 loss : 4.920353889465332\n",
      "iteration : 453 loss : 4.909339427947998\n",
      "iteration : 454 loss : 4.898545265197754\n",
      "iteration : 455 loss : 4.887901306152344\n",
      "iteration : 456 loss : 4.877554416656494\n",
      "iteration : 457 loss : 4.867377758026123\n",
      "iteration : 458 loss : 4.857357501983643\n",
      "iteration : 459 loss : 4.847464084625244\n",
      "iteration : 460 loss : 4.8376898765563965\n",
      "iteration : 461 loss : 4.828023910522461\n",
      "iteration : 462 loss : 4.818532943725586\n",
      "iteration : 463 loss : 4.809147357940674\n",
      "iteration : 464 loss : 4.799900531768799\n",
      "iteration : 465 loss : 4.790801525115967\n",
      "iteration : 466 loss : 4.781832695007324\n",
      "iteration : 467 loss : 4.772961139678955\n",
      "iteration : 468 loss : 4.764231204986572\n",
      "iteration : 469 loss : 4.75560998916626\n",
      "iteration : 470 loss : 4.747107982635498\n",
      "iteration : 471 loss : 4.738761901855469\n",
      "iteration : 472 loss : 4.730550765991211\n",
      "iteration : 473 loss : 4.722469806671143\n",
      "iteration : 474 loss : 4.714511394500732\n",
      "iteration : 475 loss : 4.706658363342285\n",
      "iteration : 476 loss : 4.698896408081055\n",
      "iteration : 477 loss : 4.691216945648193\n",
      "iteration : 478 loss : 4.683627128601074\n",
      "iteration : 479 loss : 4.676114559173584\n",
      "iteration : 480 loss : 4.668674945831299\n",
      "iteration : 481 loss : 4.661320686340332\n",
      "iteration : 482 loss : 4.654045581817627\n",
      "iteration : 483 loss : 4.646829128265381\n",
      "iteration : 484 loss : 4.639713764190674\n",
      "iteration : 485 loss : 4.6326751708984375\n",
      "iteration : 486 loss : 4.625693321228027\n",
      "iteration : 487 loss : 4.618774890899658\n",
      "iteration : 488 loss : 4.611935138702393\n",
      "iteration : 489 loss : 4.605174541473389\n",
      "iteration : 490 loss : 4.598501682281494\n",
      "iteration : 491 loss : 4.591928958892822\n",
      "iteration : 492 loss : 4.585415363311768\n",
      "iteration : 493 loss : 4.578997611999512\n",
      "iteration : 494 loss : 4.572620391845703\n",
      "iteration : 495 loss : 4.566290378570557\n",
      "iteration : 496 loss : 4.560025691986084\n",
      "iteration : 497 loss : 4.553838729858398\n",
      "iteration : 498 loss : 4.547735691070557\n",
      "iteration : 499 loss : 4.541682243347168\n",
      "iteration : 500 loss : 4.53564977645874\n",
      "iteration : 501 loss : 4.529651165008545\n",
      "iteration : 502 loss : 4.523707389831543\n",
      "iteration : 503 loss : 4.517819881439209\n",
      "iteration : 504 loss : 4.512019157409668\n",
      "iteration : 505 loss : 4.506292343139648\n",
      "iteration : 506 loss : 4.500617980957031\n",
      "iteration : 507 loss : 4.494995594024658\n",
      "iteration : 508 loss : 4.489429473876953\n",
      "iteration : 509 loss : 4.483913421630859\n",
      "iteration : 510 loss : 4.478460311889648\n",
      "iteration : 511 loss : 4.473069190979004\n",
      "iteration : 512 loss : 4.467728137969971\n",
      "iteration : 513 loss : 4.462448596954346\n",
      "iteration : 514 loss : 4.457242012023926\n",
      "iteration : 515 loss : 4.45212984085083\n",
      "iteration : 516 loss : 4.4470953941345215\n",
      "iteration : 517 loss : 4.442054748535156\n",
      "iteration : 518 loss : 4.437057971954346\n",
      "iteration : 519 loss : 4.432137489318848\n",
      "iteration : 520 loss : 4.427279949188232\n",
      "iteration : 521 loss : 4.4224853515625\n",
      "iteration : 522 loss : 4.417751789093018\n",
      "iteration : 523 loss : 4.4131011962890625\n",
      "iteration : 524 loss : 4.408528804779053\n",
      "iteration : 525 loss : 4.404049396514893\n",
      "iteration : 526 loss : 4.399611473083496\n",
      "iteration : 527 loss : 4.395207405090332\n",
      "iteration : 528 loss : 4.3908586502075195\n",
      "iteration : 529 loss : 4.386539936065674\n",
      "iteration : 530 loss : 4.382246017456055\n",
      "iteration : 531 loss : 4.377988338470459\n",
      "iteration : 532 loss : 4.373737812042236\n",
      "iteration : 533 loss : 4.369510173797607\n",
      "iteration : 534 loss : 4.3653411865234375\n",
      "iteration : 535 loss : 4.361197471618652\n",
      "iteration : 536 loss : 4.3570556640625\n",
      "iteration : 537 loss : 4.352956295013428\n",
      "iteration : 538 loss : 4.348887920379639\n",
      "iteration : 539 loss : 4.344840049743652\n",
      "iteration : 540 loss : 4.340821266174316\n",
      "iteration : 541 loss : 4.336836338043213\n",
      "iteration : 542 loss : 4.332891464233398\n",
      "iteration : 543 loss : 4.328974723815918\n",
      "iteration : 544 loss : 4.325068950653076\n",
      "iteration : 545 loss : 4.3211870193481445\n",
      "iteration : 546 loss : 4.31731653213501\n",
      "iteration : 547 loss : 4.313482284545898\n",
      "iteration : 548 loss : 4.30969762802124\n",
      "iteration : 549 loss : 4.305940628051758\n",
      "iteration : 550 loss : 4.3022141456604\n",
      "iteration : 551 loss : 4.298523426055908\n",
      "iteration : 552 loss : 4.294867992401123\n",
      "iteration : 553 loss : 4.291242599487305\n",
      "iteration : 554 loss : 4.2876105308532715\n",
      "iteration : 555 loss : 4.283957004547119\n",
      "iteration : 556 loss : 4.280341148376465\n",
      "iteration : 557 loss : 4.2767486572265625\n",
      "iteration : 558 loss : 4.273245334625244\n",
      "iteration : 559 loss : 4.269766330718994\n",
      "iteration : 560 loss : 4.266293525695801\n",
      "iteration : 561 loss : 4.262869834899902\n",
      "iteration : 562 loss : 4.259474754333496\n",
      "iteration : 563 loss : 4.2560954093933105\n",
      "iteration : 564 loss : 4.252749443054199\n",
      "iteration : 565 loss : 4.249460697174072\n",
      "iteration : 566 loss : 4.2461957931518555\n",
      "iteration : 567 loss : 4.2429375648498535\n",
      "iteration : 568 loss : 4.239686489105225\n",
      "iteration : 569 loss : 4.236468315124512\n",
      "iteration : 570 loss : 4.233278274536133\n",
      "iteration : 571 loss : 4.23011589050293\n",
      "iteration : 572 loss : 4.226983070373535\n",
      "iteration : 573 loss : 4.2238688468933105\n",
      "iteration : 574 loss : 4.220782279968262\n",
      "iteration : 575 loss : 4.217698574066162\n",
      "iteration : 576 loss : 4.214585304260254\n",
      "iteration : 577 loss : 4.211481094360352\n",
      "iteration : 578 loss : 4.208390712738037\n",
      "iteration : 579 loss : 4.205306053161621\n",
      "iteration : 580 loss : 4.202226161956787\n",
      "iteration : 581 loss : 4.199154853820801\n",
      "iteration : 582 loss : 4.196098327636719\n",
      "iteration : 583 loss : 4.193048000335693\n",
      "iteration : 584 loss : 4.189988613128662\n",
      "iteration : 585 loss : 4.186906337738037\n",
      "iteration : 586 loss : 4.183859348297119\n",
      "iteration : 587 loss : 4.180824279785156\n",
      "iteration : 588 loss : 4.177812576293945\n",
      "iteration : 589 loss : 4.174797058105469\n",
      "iteration : 590 loss : 4.171794891357422\n",
      "iteration : 591 loss : 4.1688103675842285\n",
      "iteration : 592 loss : 4.16584587097168\n",
      "iteration : 593 loss : 4.162834644317627\n",
      "iteration : 594 loss : 4.159815311431885\n",
      "iteration : 595 loss : 4.156787872314453\n",
      "iteration : 596 loss : 4.153717517852783\n",
      "iteration : 597 loss : 4.150610446929932\n",
      "iteration : 598 loss : 4.1474809646606445\n",
      "iteration : 599 loss : 4.144317150115967\n",
      "iteration : 600 loss : 4.1411027908325195\n",
      "iteration : 601 loss : 4.137803554534912\n",
      "iteration : 602 loss : 4.134481430053711\n",
      "iteration : 603 loss : 4.13114595413208\n",
      "iteration : 604 loss : 4.1277852058410645\n",
      "iteration : 605 loss : 4.1244635581970215\n",
      "iteration : 606 loss : 4.121125221252441\n",
      "iteration : 607 loss : 4.117791175842285\n",
      "iteration : 608 loss : 4.114490985870361\n",
      "iteration : 609 loss : 4.1111836433410645\n",
      "iteration : 610 loss : 4.10784387588501\n",
      "iteration : 611 loss : 4.104511737823486\n",
      "iteration : 612 loss : 4.101132869720459\n",
      "iteration : 613 loss : 4.0977582931518555\n",
      "iteration : 614 loss : 4.094359397888184\n",
      "iteration : 615 loss : 4.090931415557861\n",
      "iteration : 616 loss : 4.0875468254089355\n",
      "iteration : 617 loss : 4.084190368652344\n",
      "iteration : 618 loss : 4.080807209014893\n",
      "iteration : 619 loss : 4.077385902404785\n",
      "iteration : 620 loss : 4.073920726776123\n",
      "iteration : 621 loss : 4.070411205291748\n",
      "iteration : 622 loss : 4.066850662231445\n",
      "iteration : 623 loss : 4.063227653503418\n",
      "iteration : 624 loss : 4.059546947479248\n",
      "iteration : 625 loss : 4.055891990661621\n",
      "iteration : 626 loss : 4.052272319793701\n",
      "iteration : 627 loss : 4.04863977432251\n",
      "iteration : 628 loss : 4.044979572296143\n",
      "iteration : 629 loss : 4.041332244873047\n",
      "iteration : 630 loss : 4.03776216506958\n",
      "iteration : 631 loss : 4.034219741821289\n",
      "iteration : 632 loss : 4.030745506286621\n",
      "iteration : 633 loss : 4.027353286743164\n",
      "iteration : 634 loss : 4.024041175842285\n",
      "iteration : 635 loss : 4.020702838897705\n",
      "iteration : 636 loss : 4.017387390136719\n",
      "iteration : 637 loss : 4.0140581130981445\n",
      "iteration : 638 loss : 4.010718822479248\n",
      "iteration : 639 loss : 4.007379055023193\n",
      "iteration : 640 loss : 4.004060745239258\n",
      "iteration : 641 loss : 4.0008039474487305\n",
      "iteration : 642 loss : 3.997572183609009\n",
      "iteration : 643 loss : 3.9942896366119385\n",
      "iteration : 644 loss : 3.9910261631011963\n",
      "iteration : 645 loss : 3.9878597259521484\n",
      "iteration : 646 loss : 3.984734058380127\n",
      "iteration : 647 loss : 3.981699228286743\n",
      "iteration : 648 loss : 3.978692054748535\n",
      "iteration : 649 loss : 3.9756908416748047\n",
      "iteration : 650 loss : 3.972726345062256\n",
      "iteration : 651 loss : 3.9697682857513428\n",
      "iteration : 652 loss : 3.9668068885803223\n",
      "iteration : 653 loss : 3.963864803314209\n",
      "iteration : 654 loss : 3.9609525203704834\n",
      "iteration : 655 loss : 3.9580843448638916\n",
      "iteration : 656 loss : 3.955202341079712\n",
      "iteration : 657 loss : 3.952342987060547\n",
      "iteration : 658 loss : 3.9495022296905518\n",
      "iteration : 659 loss : 3.9466750621795654\n",
      "iteration : 660 loss : 3.9438769817352295\n",
      "iteration : 661 loss : 3.941131830215454\n",
      "iteration : 662 loss : 3.938398599624634\n",
      "iteration : 663 loss : 3.9356801509857178\n",
      "iteration : 664 loss : 3.932981252670288\n",
      "iteration : 665 loss : 3.9302971363067627\n",
      "iteration : 666 loss : 3.9276301860809326\n",
      "iteration : 667 loss : 3.9249866008758545\n",
      "iteration : 668 loss : 3.922348976135254\n",
      "iteration : 669 loss : 3.9197206497192383\n",
      "iteration : 670 loss : 3.9170992374420166\n",
      "iteration : 671 loss : 3.9144999980926514\n",
      "iteration : 672 loss : 3.9119203090667725\n",
      "iteration : 673 loss : 3.9093592166900635\n",
      "iteration : 674 loss : 3.9068195819854736\n",
      "iteration : 675 loss : 3.904278516769409\n",
      "iteration : 676 loss : 3.9017481803894043\n",
      "iteration : 677 loss : 3.8992490768432617\n",
      "iteration : 678 loss : 3.8967573642730713\n",
      "iteration : 679 loss : 3.894263982772827\n",
      "iteration : 680 loss : 3.891771078109741\n",
      "iteration : 681 loss : 3.8892972469329834\n",
      "iteration : 682 loss : 3.886847734451294\n",
      "iteration : 683 loss : 3.884442090988159\n",
      "iteration : 684 loss : 3.8820765018463135\n",
      "iteration : 685 loss : 3.87971568107605\n",
      "iteration : 686 loss : 3.877363443374634\n",
      "iteration : 687 loss : 3.8750133514404297\n",
      "iteration : 688 loss : 3.8726882934570312\n",
      "iteration : 689 loss : 3.870394229888916\n",
      "iteration : 690 loss : 3.86812424659729\n",
      "iteration : 691 loss : 3.8658854961395264\n",
      "iteration : 692 loss : 3.863654613494873\n",
      "iteration : 693 loss : 3.861414909362793\n",
      "iteration : 694 loss : 3.859182834625244\n",
      "iteration : 695 loss : 3.856950283050537\n",
      "iteration : 696 loss : 3.8547542095184326\n",
      "iteration : 697 loss : 3.852565050125122\n",
      "iteration : 698 loss : 3.8503732681274414\n",
      "iteration : 699 loss : 3.8481805324554443\n",
      "iteration : 700 loss : 3.845991849899292\n",
      "iteration : 701 loss : 3.8438503742218018\n",
      "iteration : 702 loss : 3.8417587280273438\n",
      "iteration : 703 loss : 3.8396642208099365\n",
      "iteration : 704 loss : 3.837571859359741\n",
      "iteration : 705 loss : 3.8355066776275635\n",
      "iteration : 706 loss : 3.833451747894287\n",
      "iteration : 707 loss : 3.831423044204712\n",
      "iteration : 708 loss : 3.829430341720581\n",
      "iteration : 709 loss : 3.8274455070495605\n",
      "iteration : 710 loss : 3.8254783153533936\n",
      "iteration : 711 loss : 3.823538303375244\n",
      "iteration : 712 loss : 3.8216049671173096\n",
      "iteration : 713 loss : 3.819666862487793\n",
      "iteration : 714 loss : 3.8177363872528076\n",
      "iteration : 715 loss : 3.8158082962036133\n",
      "iteration : 716 loss : 3.8138790130615234\n",
      "iteration : 717 loss : 3.811951160430908\n",
      "iteration : 718 loss : 3.809997081756592\n",
      "iteration : 719 loss : 3.8080739974975586\n",
      "iteration : 720 loss : 3.8061699867248535\n",
      "iteration : 721 loss : 3.8042831420898438\n",
      "iteration : 722 loss : 3.8023815155029297\n",
      "iteration : 723 loss : 3.800499439239502\n",
      "iteration : 724 loss : 3.7986278533935547\n",
      "iteration : 725 loss : 3.7967512607574463\n",
      "iteration : 726 loss : 3.794870376586914\n",
      "iteration : 727 loss : 3.7929909229278564\n",
      "iteration : 728 loss : 3.7911131381988525\n",
      "iteration : 729 loss : 3.789257287979126\n",
      "iteration : 730 loss : 3.7874295711517334\n",
      "iteration : 731 loss : 3.785598039627075\n",
      "iteration : 732 loss : 3.7837769985198975\n",
      "iteration : 733 loss : 3.781970500946045\n",
      "iteration : 734 loss : 3.7801811695098877\n",
      "iteration : 735 loss : 3.7784125804901123\n",
      "iteration : 736 loss : 3.776654005050659\n",
      "iteration : 737 loss : 3.774906635284424\n",
      "iteration : 738 loss : 3.773169755935669\n",
      "iteration : 739 loss : 3.7714576721191406\n",
      "iteration : 740 loss : 3.7697691917419434\n",
      "iteration : 741 loss : 3.768085479736328\n",
      "iteration : 742 loss : 3.766411066055298\n",
      "iteration : 743 loss : 3.7647666931152344\n",
      "iteration : 744 loss : 3.76313853263855\n",
      "iteration : 745 loss : 3.761517286300659\n",
      "iteration : 746 loss : 3.7599000930786133\n",
      "iteration : 747 loss : 3.7582743167877197\n",
      "iteration : 748 loss : 3.7566487789154053\n",
      "iteration : 749 loss : 3.7550275325775146\n",
      "iteration : 750 loss : 3.7534170150756836\n",
      "iteration : 751 loss : 3.7518117427825928\n",
      "iteration : 752 loss : 3.7502105236053467\n",
      "iteration : 753 loss : 3.7486073970794678\n",
      "iteration : 754 loss : 3.7470169067382812\n",
      "iteration : 755 loss : 3.7454326152801514\n",
      "iteration : 756 loss : 3.743861675262451\n",
      "iteration : 757 loss : 3.7423033714294434\n",
      "iteration : 758 loss : 3.74075984954834\n",
      "iteration : 759 loss : 3.7392101287841797\n",
      "iteration : 760 loss : 3.737654685974121\n",
      "iteration : 761 loss : 3.7361068725585938\n",
      "iteration : 762 loss : 3.73456072807312\n",
      "iteration : 763 loss : 3.7330377101898193\n",
      "iteration : 764 loss : 3.73153018951416\n",
      "iteration : 765 loss : 3.7300236225128174\n",
      "iteration : 766 loss : 3.7285237312316895\n",
      "iteration : 767 loss : 3.7270307540893555\n",
      "iteration : 768 loss : 3.7255353927612305\n",
      "iteration : 769 loss : 3.7240381240844727\n",
      "iteration : 770 loss : 3.7225465774536133\n",
      "iteration : 771 loss : 3.7210569381713867\n",
      "iteration : 772 loss : 3.7195751667022705\n",
      "iteration : 773 loss : 3.7181079387664795\n",
      "iteration : 774 loss : 3.716630697250366\n",
      "iteration : 775 loss : 3.715141534805298\n",
      "iteration : 776 loss : 3.713663101196289\n",
      "iteration : 777 loss : 3.7121760845184326\n",
      "iteration : 778 loss : 3.7106895446777344\n",
      "iteration : 779 loss : 3.7092080116271973\n",
      "iteration : 780 loss : 3.707733154296875\n",
      "iteration : 781 loss : 3.706277370452881\n",
      "iteration : 782 loss : 3.7048227787017822\n",
      "iteration : 783 loss : 3.7033655643463135\n",
      "iteration : 784 loss : 3.7019214630126953\n",
      "iteration : 785 loss : 3.7004950046539307\n",
      "iteration : 786 loss : 3.6990654468536377\n",
      "iteration : 787 loss : 3.697603464126587\n",
      "iteration : 788 loss : 3.6961426734924316\n",
      "iteration : 789 loss : 3.6946871280670166\n",
      "iteration : 790 loss : 3.6932404041290283\n",
      "iteration : 791 loss : 3.6917953491210938\n",
      "iteration : 792 loss : 3.6903531551361084\n",
      "iteration : 793 loss : 3.68892240524292\n",
      "iteration : 794 loss : 3.6874892711639404\n",
      "iteration : 795 loss : 3.68605375289917\n",
      "iteration : 796 loss : 3.684619665145874\n",
      "iteration : 797 loss : 3.6831979751586914\n",
      "iteration : 798 loss : 3.681802272796631\n",
      "iteration : 799 loss : 3.680418014526367\n",
      "iteration : 800 loss : 3.6790425777435303\n",
      "iteration : 801 loss : 3.677671432495117\n",
      "iteration : 802 loss : 3.6763076782226562\n",
      "iteration : 803 loss : 3.6749374866485596\n",
      "iteration : 804 loss : 3.6735687255859375\n",
      "iteration : 805 loss : 3.6722071170806885\n",
      "iteration : 806 loss : 3.6708602905273438\n",
      "iteration : 807 loss : 3.6695289611816406\n",
      "iteration : 808 loss : 3.668208599090576\n",
      "iteration : 809 loss : 3.6668992042541504\n",
      "iteration : 810 loss : 3.6655941009521484\n",
      "iteration : 811 loss : 3.664290189743042\n",
      "iteration : 812 loss : 3.6629886627197266\n",
      "iteration : 813 loss : 3.6616883277893066\n",
      "iteration : 814 loss : 3.6603901386260986\n",
      "iteration : 815 loss : 3.65909743309021\n",
      "iteration : 816 loss : 3.657813549041748\n",
      "iteration : 817 loss : 3.6565325260162354\n",
      "iteration : 818 loss : 3.6552560329437256\n",
      "iteration : 819 loss : 3.653982400894165\n",
      "iteration : 820 loss : 3.6527140140533447\n",
      "iteration : 821 loss : 3.6514551639556885\n",
      "iteration : 822 loss : 3.6502020359039307\n",
      "iteration : 823 loss : 3.648958921432495\n",
      "iteration : 824 loss : 3.6477181911468506\n",
      "iteration : 825 loss : 3.646489381790161\n",
      "iteration : 826 loss : 3.645270586013794\n",
      "iteration : 827 loss : 3.644062042236328\n",
      "iteration : 828 loss : 3.6428587436676025\n",
      "iteration : 829 loss : 3.641659736633301\n",
      "iteration : 830 loss : 3.6404669284820557\n",
      "iteration : 831 loss : 3.6392838954925537\n",
      "iteration : 832 loss : 3.6381053924560547\n",
      "iteration : 833 loss : 3.636927366256714\n",
      "iteration : 834 loss : 3.635755777359009\n",
      "iteration : 835 loss : 3.6345865726470947\n",
      "iteration : 836 loss : 3.633420705795288\n",
      "iteration : 837 loss : 3.6322617530822754\n",
      "iteration : 838 loss : 3.6311089992523193\n",
      "iteration : 839 loss : 3.6299636363983154\n",
      "iteration : 840 loss : 3.6288182735443115\n",
      "iteration : 841 loss : 3.6276769638061523\n",
      "iteration : 842 loss : 3.6265389919281006\n",
      "iteration : 843 loss : 3.625404119491577\n",
      "iteration : 844 loss : 3.6242740154266357\n",
      "iteration : 845 loss : 3.623152256011963\n",
      "iteration : 846 loss : 3.622036933898926\n",
      "iteration : 847 loss : 3.620924234390259\n",
      "iteration : 848 loss : 3.6198153495788574\n",
      "iteration : 849 loss : 3.618704080581665\n",
      "iteration : 850 loss : 3.617601156234741\n",
      "iteration : 851 loss : 3.6165030002593994\n",
      "iteration : 852 loss : 3.615410566329956\n",
      "iteration : 853 loss : 3.614320993423462\n",
      "iteration : 854 loss : 3.613238573074341\n",
      "iteration : 855 loss : 3.6121633052825928\n",
      "iteration : 856 loss : 3.6110920906066895\n",
      "iteration : 857 loss : 3.610025644302368\n",
      "iteration : 858 loss : 3.608964681625366\n",
      "iteration : 859 loss : 3.6079084873199463\n",
      "iteration : 860 loss : 3.606858968734741\n",
      "iteration : 861 loss : 3.6058154106140137\n",
      "iteration : 862 loss : 3.604778289794922\n",
      "iteration : 863 loss : 3.6037516593933105\n",
      "iteration : 864 loss : 3.6027331352233887\n",
      "iteration : 865 loss : 3.6017239093780518\n",
      "iteration : 866 loss : 3.6007165908813477\n",
      "iteration : 867 loss : 3.599712610244751\n",
      "iteration : 868 loss : 3.5987143516540527\n",
      "iteration : 869 loss : 3.5977237224578857\n",
      "iteration : 870 loss : 3.5967397689819336\n",
      "iteration : 871 loss : 3.5957489013671875\n",
      "iteration : 872 loss : 3.5947659015655518\n",
      "iteration : 873 loss : 3.593784809112549\n",
      "iteration : 874 loss : 3.592806100845337\n",
      "iteration : 875 loss : 3.5918333530426025\n",
      "iteration : 876 loss : 3.5908710956573486\n",
      "iteration : 877 loss : 3.589918375015259\n",
      "iteration : 878 loss : 3.5889742374420166\n",
      "iteration : 879 loss : 3.5880367755889893\n",
      "iteration : 880 loss : 3.587104558944702\n",
      "iteration : 881 loss : 3.586179733276367\n",
      "iteration : 882 loss : 3.585259437561035\n",
      "iteration : 883 loss : 3.5843443870544434\n",
      "iteration : 884 loss : 3.5834295749664307\n",
      "iteration : 885 loss : 3.5825161933898926\n",
      "iteration : 886 loss : 3.5816104412078857\n",
      "iteration : 887 loss : 3.5807013511657715\n",
      "iteration : 888 loss : 3.579794406890869\n",
      "iteration : 889 loss : 3.5788893699645996\n",
      "iteration : 890 loss : 3.5779850482940674\n",
      "iteration : 891 loss : 3.5770843029022217\n",
      "iteration : 892 loss : 3.5761868953704834\n",
      "iteration : 893 loss : 3.575286865234375\n",
      "iteration : 894 loss : 3.5743892192840576\n",
      "iteration : 895 loss : 3.5734941959381104\n",
      "iteration : 896 loss : 3.5725998878479004\n",
      "iteration : 897 loss : 3.5717039108276367\n",
      "iteration : 898 loss : 3.5708131790161133\n",
      "iteration : 899 loss : 3.5699222087860107\n",
      "iteration : 900 loss : 3.569030523300171\n",
      "iteration : 901 loss : 3.5681440830230713\n",
      "iteration : 902 loss : 3.5672607421875\n",
      "iteration : 903 loss : 3.5663821697235107\n",
      "iteration : 904 loss : 3.5655081272125244\n",
      "iteration : 905 loss : 3.5646393299102783\n",
      "iteration : 906 loss : 3.563774347305298\n",
      "iteration : 907 loss : 3.5629143714904785\n",
      "iteration : 908 loss : 3.5620555877685547\n",
      "iteration : 909 loss : 3.5611979961395264\n",
      "iteration : 910 loss : 3.5603432655334473\n",
      "iteration : 911 loss : 3.5594851970672607\n",
      "iteration : 912 loss : 3.5586326122283936\n",
      "iteration : 913 loss : 3.5577855110168457\n",
      "iteration : 914 loss : 3.556945323944092\n",
      "iteration : 915 loss : 3.556105136871338\n",
      "iteration : 916 loss : 3.555267333984375\n",
      "iteration : 917 loss : 3.5544347763061523\n",
      "iteration : 918 loss : 3.553602695465088\n",
      "iteration : 919 loss : 3.5527777671813965\n",
      "iteration : 920 loss : 3.551955223083496\n",
      "iteration : 921 loss : 3.5511348247528076\n",
      "iteration : 922 loss : 3.5503146648406982\n",
      "iteration : 923 loss : 3.54949688911438\n",
      "iteration : 924 loss : 3.548682451248169\n",
      "iteration : 925 loss : 3.547869920730591\n",
      "iteration : 926 loss : 3.547063112258911\n",
      "iteration : 927 loss : 3.546259641647339\n",
      "iteration : 928 loss : 3.5454611778259277\n",
      "iteration : 929 loss : 3.544661283493042\n",
      "iteration : 930 loss : 3.54386043548584\n",
      "iteration : 931 loss : 3.5430643558502197\n",
      "iteration : 932 loss : 3.5422706604003906\n",
      "iteration : 933 loss : 3.541483163833618\n",
      "iteration : 934 loss : 3.540698528289795\n",
      "iteration : 935 loss : 3.539917469024658\n",
      "iteration : 936 loss : 3.5391387939453125\n",
      "iteration : 937 loss : 3.53836727142334\n",
      "iteration : 938 loss : 3.5376012325286865\n",
      "iteration : 939 loss : 3.5368380546569824\n",
      "iteration : 940 loss : 3.536072254180908\n",
      "iteration : 941 loss : 3.535309314727783\n",
      "iteration : 942 loss : 3.534543991088867\n",
      "iteration : 943 loss : 3.533784866333008\n",
      "iteration : 944 loss : 3.5330235958099365\n",
      "iteration : 945 loss : 3.5322675704956055\n",
      "iteration : 946 loss : 3.531511068344116\n",
      "iteration : 947 loss : 3.5307586193084717\n",
      "iteration : 948 loss : 3.5300121307373047\n",
      "iteration : 949 loss : 3.5292584896087646\n",
      "iteration : 950 loss : 3.528507709503174\n",
      "iteration : 951 loss : 3.5277624130249023\n",
      "iteration : 952 loss : 3.527014970779419\n",
      "iteration : 953 loss : 3.5262722969055176\n",
      "iteration : 954 loss : 3.5255305767059326\n",
      "iteration : 955 loss : 3.524794340133667\n",
      "iteration : 956 loss : 3.5240585803985596\n",
      "iteration : 957 loss : 3.5233285427093506\n",
      "iteration : 958 loss : 3.5225982666015625\n",
      "iteration : 959 loss : 3.5218753814697266\n",
      "iteration : 960 loss : 3.5211551189422607\n",
      "iteration : 961 loss : 3.520434617996216\n",
      "iteration : 962 loss : 3.5197184085845947\n",
      "iteration : 963 loss : 3.5190088748931885\n",
      "iteration : 964 loss : 3.5182995796203613\n",
      "iteration : 965 loss : 3.5175864696502686\n",
      "iteration : 966 loss : 3.5168821811676025\n",
      "iteration : 967 loss : 3.516183376312256\n",
      "iteration : 968 loss : 3.5154855251312256\n",
      "iteration : 969 loss : 3.514788866043091\n",
      "iteration : 970 loss : 3.5140933990478516\n",
      "iteration : 971 loss : 3.513404130935669\n",
      "iteration : 972 loss : 3.512721300125122\n",
      "iteration : 973 loss : 3.5120487213134766\n",
      "iteration : 974 loss : 3.511382579803467\n",
      "iteration : 975 loss : 3.5107228755950928\n",
      "iteration : 976 loss : 3.5100669860839844\n",
      "iteration : 977 loss : 3.509413957595825\n",
      "iteration : 978 loss : 3.5087642669677734\n",
      "iteration : 979 loss : 3.508120059967041\n",
      "iteration : 980 loss : 3.507474899291992\n",
      "iteration : 981 loss : 3.506833553314209\n",
      "iteration : 982 loss : 3.5061943531036377\n",
      "iteration : 983 loss : 3.505561351776123\n",
      "iteration : 984 loss : 3.5049350261688232\n",
      "iteration : 985 loss : 3.504312038421631\n",
      "iteration : 986 loss : 3.5036919116973877\n",
      "iteration : 987 loss : 3.503077983856201\n",
      "iteration : 988 loss : 3.502459764480591\n",
      "iteration : 989 loss : 3.5018467903137207\n",
      "iteration : 990 loss : 3.5012426376342773\n",
      "iteration : 991 loss : 3.5006399154663086\n",
      "iteration : 992 loss : 3.5000417232513428\n",
      "iteration : 993 loss : 3.4994499683380127\n",
      "iteration : 994 loss : 3.4988584518432617\n",
      "iteration : 995 loss : 3.4982666969299316\n",
      "iteration : 996 loss : 3.4976816177368164\n",
      "iteration : 997 loss : 3.4970991611480713\n",
      "iteration : 998 loss : 3.496511936187744\n",
      "iteration : 999 loss : 3.495929718017578\n"
     ]
    }
   ],
   "source": [
    "#training loop\n",
    "\n",
    "n=1000\n",
    "model.train()\n",
    "for epoch in range(n):\n",
    "    optimizer.zero_grad()\n",
    "    output=model(Xtrain)\n",
    "    loss=criterion(output,ytrain)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(\"iteration :\",epoch,\"loss :\",loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 3.6354188919067383\n"
     ]
    }
   ],
   "source": [
    "#test loop\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction=model(Xtest)\n",
    "    testloss=criterion(prediction,ytest)\n",
    "    print(\"Loss :\",testloss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
